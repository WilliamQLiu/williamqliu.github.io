---
layout: post
title: Hadoop 
---

## {{ page.title }}

- - - -

##Table of Contents

*  [Summary](#summary)
  -  [Why Hadoop](#whyhadoop)
*  [How does Hadoop work?](#howworks)
  -  [MapReduce](#mapreduce)
  -  [HDFS](#hdfs)
*  [Hadoop v2 Ecosystem (Pig, Hive, Spark)](#diffhadoopversions)
  -  [Overview of Layers](#overviewlayers)
  -  [Hive](#hive)
  -  [Pig](#pig)
  -  [Spark](#spark)
  -  [Hadoop User Interface (HUE)](#hue)
*  [Hello Hadoop Code](#hellohadoop)
*  [Amazon Elastic Map Reduce](#emr)
  -  [Data Layers](#datalayers)
  -  [Data Architectures](#datasetups)
  -  [How to execute work with EMR](#emrexecutework)
  -  [EMR setup security](#emrsecurity)
*  [Amazon Data Pipeline for ETL](#awsdatapipeline)
  -  [General Pipeline Commands](#pipelinebasiccmds)
  -  [Use Case: Clickstreams](#pipelineusecase)


##<a id="summary">Summary</a>

Apache's __Hadoop__ is an open source software framework for big and/or unstructured data that is programmed in Java.  This means that if you're working with very large data sets (e.g. 100 Terabytes), then you'll need Hadoop.  It's important to note that if you don't work with very large data sets or unstructured data, then you probably do NOT need Hadoop.  You're probably better off with using a standard relational SQL database.

####<a id="whyhadoop">Why Hadoop</a>

So why Hadoop?  Instead of running on a single powerful server, Hadoop can run on a large cluster of commodity hardware.  This means that a network of personal computers (nodes) can coordinate their processing power and storage (see HDFS below).  Hadoop allows systems to build __horizontally__ (i.e. more computers) instead of just __vertically__ (i.e. a faster computer).

Hadoop is the right system for 'Big Data'.  Choose Hadoop if a lot of this applies:

*  Data lacks structure
*  Analyzing streams of information
*  Processing large datasets
*  Warehousing large datasets
*  Flexibility for ad hoc analysis
*  Speed of queries on large data sets

Hadoop is good for:

*  Massively parallel
*  Scalable and fault tolerant
*  Flixibility for multiple languages and data formats
*  Open Source
*  Ecosystem of tools
*  Batch and real-time analytics

##<a id="howworks">How does Hadoop work?</a>

Hadoop's architecture is built from Google's __MapReduce__ and the __Google File System__ white papers so it is no surprise that Hadoop consists of __MapReduce__ and the __Hadoop File System (HDFS)__.

####<a id="mapreduce">MapReduce</a>

MapReduce is broken down into two pieces, __mappers__ and __reducers__:

*  mappers load data from the HDFS and filters, transforms, parses, and outputs (key, value) pairs
*  reducers automatically groups by the mapper's output key and summarizes (say aggregates, count) the HDFS

The general steps look like this:

1. split
2. map
3. sort/shuffle (this is done automatically)
4. reduce

Note: There are specific MapReduce programs that allow higher level querying like Pig (that allow GROUP BY, FOREACH statements), Hive (similar to SQL, but less powerful), and Mahout (a Machine Learning language library that sometimes uses Hadoop for recommending, clustering, classification).


####<a id="hdfs">HDFS</a>

The __Hadoop File System (HDFS)__ provides __redundancy__ and __fault-tolerant__ storage by breaking data into chunks of about 64MB - 2GB, then it creates 3 instances of the same data, and spreads it across a network of computers. Say that one of the networked computers has a piece of data you need and it goes down; there are still two other copies of the data on the network that is readily available. This is much different than many enterprise systems where if a major server goes down, it would take anywhere from minutes to hours or days to fully restore.

Note: There are specific HDFS data systems like HBase and Accumulo that allow you to fetch keys quickly, which are good for transactional systems.

##<a id="diffhadoopversions">Hadoop v2 ecosystem</a>

There were a few changes from Hadoop v1 to Hadoop v2, mainly the addition of YARN and a lot more data processing applications like Pig, Hive, etc.

__Hadoop 1__

Main pieces of Hadoop 1.0 were MapReduce sitting on top of the HDFS.

__Hadoop 2__

With Hadoop 2.0, we still have MapReduce and HDFS, but now also have an additional layer __YARN__ that acts as a resource manager for distributed applications; YARN sits between the MapReduce and HDFS layers.  Client submits job to YARN Resource Manager, which then automatically distributes and manages the job.  Along with MapReduce, we have a few other data processing like Hive, Pig, Spark, etc.

####<a id="overviewlayers">Overview of Hadoop 2 Layers</a>

1. Applications with Pig, Hive, Cascading, Mahout, Giraph, Presto
2. Batch jobs with MapReduce; Interactive with Tez; In memory with Spark
3. YARN for Cluster Resource Management
4. Storage with S3, HDFS

####<a id="hive">Hive</a>

Use Hive to interact with your data in HDFS and Amazon S3

*  Batch or ad-hoc workloads
*  SQL-like query language (HiveQL) to allow users with knowledge of SQL to leverage Hadoop
*  Schema-on-read to query data without needing pre-processing using the Hive metastore.  You basically create your table however you want and say here's how Hive should see the table using the _metastore_.

####<a id="pig">Pig</a>

*  Uses high level 'Pig latin' language to easily script data transformations in Hadoop
*  Strong optimizer for workloads

####<a id="spark">Spark</a>

An alternative to MapReduce.  Spark uses a __Directed Acyclic Graph__ instead of Hadoop's Map-Reduce.  Has very high performance, but is in memory.

*  In-memory for fast queries
*  Great for machine learning (MLlib) or other iterative queries
*  Use Spark SQL to create a low-latency data warehouse
*  Spark Streaming library for real-time, stream processing workloads
*  Runs on YARN (and other cluster managemers too)

####<a id="hue">Hadoop User Experience (HUE) GUI for Hive and Pig</a>

Can interact in an ad-hoc way with the HUE GUI.

####<a id="hellohadoop">Hello World of Hadoop</a>

__Notes__

Books: Hadoop the Definitive Guide
Streaming with Python just use `stdin` and `stdout`
*  Login to server through ssh.  E.g. `ssh username@216.230.228.88`, then enter your password for the ssh.
*  Directory 'streaming-examples' has code for stock prices, wordcount, and word frequencies.  In each directory, enter this in the command line to run your hadoop code: `source run-hadoop.sh`
*  Output gets stored in output/part-000000 should match file expected-output.

__run-hadoop.sh__

Run hadoop using: `source run-hadoop.sh`


    hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-*.jar \
      -input ./input.txt \
      -output ./output \
      -mapper map.py \
      -reducer reduce.py

__input.txt__

    Goog, 230, 240
    Apple, 100, 98
    MS, 300, 250
    MS, 250, 260
    MS, 270, 280
    Goog, 220, 215
    Goog, 300, 350
    IBM, 80, 90
    IBM, 90, 85

__map.py__

    #!/usr/bin/env python
    import sys
    import string
    
    for line in sys.stdin:
        record = line.split(",")
        opening = int(record[1])
        closing = int(record[2])
        if (closing > opening):
            change = float(closing - opening) / opening
            print '%s\t%s' % (record[0], change)

__reduce.py__

    #!/usr/bin/env python
    import sys
    import string
    
    stock = None
    max_increase = 0
    for line in sys.stdin:
       next_stock, increase = line.split('\t')
       increase = float(increase)
       if next_stock == stock:     # another line for the same stock
           if increase > max_increase:
               max_increase = increase
       else:  # new stock; output result for previous stock
           if stock != None:  # only false on the very first line of input
               print( "%s\t%f" % (stock, max_increase) )
           stock = next_stock
           max_increase = increase
    # print the last
    print( "%s\t%f" % (stock, max_increase) )    

__expected-output__

    Goog    0.166667
    IBM     0.125000
    MS      0.040000

__output/part-00000__

This is created after running the `source run-hadoop.sh`

    Goog    0.166667
    IBM     0.125000
    MS      0.040000


##<a id="emr">Amazon Elastic Map Reduce</a>

*  Launch a cluster in minutes
*  Low cost with hourly rate
*  Elastic, easily add or remove capacity
*  EMR automatically rebalances tasks if servers drop
*  Can easily resize clusters
*  Can also have EMR push logs to S3 easily
*  Can launch in AWS Management Console, AWS Command Line Interface, or Amazon EMR API.
*  Security taken care of by AWS Identity and Access Management (IAM)

In multiple EMR instance groups, we have:

*  Master Node (submit jobs to this)
*  Slave Group - Core (runs data managers like YARN)
*  Slave Group - Task (just runs node manager, does tasks; this is what you want your spot instances to be running in case it shuts down if you lose bid for server)

####<a id="datalayers">S3 and HDFS as your data layers</a>

*  EMR File System (EMRFS) to access objects in S3
  -  Decouple your storage layer from your cluster
  -  Leverage S3's durability
  -  Good performance for sequential reads (which is common in analytics workloads)
*  Hadoop Distributed File System (HDFS)
  -  3x replication for durability
  -  Uses the local disk from your EC2 instances in your cluster

####<a id="datasetups">Data Architectures</a>

You can architect your data a few different ways.  Here are a few examples:

1. Data (e.g. GB of logs) are pushed to S3
2a. You can either push out a small amount of data to a local server
2b. If you don't want to save data in the local server, you can just use S3 instead of HDFS for your data layer to decouple your compute capacity and storage.

__Long-running cluster__

*  Daily EMR cluster ETL data into database with Pig
*  24/7 EMR cluster running HBase

__Interactive query__

*  Hive metastore on Amazon EMR, then use business intelligence tools for ad-hoc investigation like cloudera impala, spark, presto, tez, and hive

__EMR for ETL and query engine for investigations__

This takes the S3 data and splits it two ways:

1. Spark for transient EMR cluster for ad-hoc analysis of entire log set
2. Hourly EMR cluster using Spark for ETL, then load subset into Redshift Data Warehouse

Other interesting ETL setups include:

*  Nasdaq Data Lake Architecture
*  Washington Post and Spark on EMR

__Streaming Data Processing__

Logs stored in Amazon Kinesis, then it splits out to:

*  Amazon Kinesis Client Library
*  AWS Lambda
*  Spark Streaming
*  Amazon EMR with Hive, Pig, Cascading
*  Amazon EC2 and Storm


####<a id="emrexecutework">How to use execute work on EMR</a>

You can either:

*  via EMR Step API
*  Connect to Master Node and then work directly in shell to submit work or connect to HUE.

You can have a variety of data stores including:

*  AWS S3
*  HDFS
*  AWS DynamoDB
*  AWS Redshift
*  AWS Glacier
*  AWS RDS

####<a id="emrsecurity">How to set security for EMR</a>

EMR uses two IAM roles for security:

*  EMR service role is for the EMR control panel
*  EC2 instance profile is for the actual instances in the Amazon EMR cluster

EMR by default creates two security groups:

*  Master Security Group - has port 22 access for SSHing into your cluster
*  Slave Security Group - is a single default master and default slave security group across all of your clusters
*  You can add additional security groups to the master and slave groups on a cluster to separate them further

Bootstrap Actions configures your applications (e.g. setup `core-site.xml`, `hdfs-site.xml`, `mapreduce.xml`, `yarn.xml`)

You should consider using client-side encrypted objects in S3.  Also should compress your data files.  S3 can be used as Landing Zone and/or as Data Lake.

EMR is also HIPAA-eligible.

##<a id="awsdatapipeline">AWS Data Pipeline for ETL</a>

AWS Data Pipeline, can access through the console, command line interface, APIs

*  Manages your workflow data orchestration based on activities you define
*  Manages dependencies and automated scheduling for you
*  Provides notifications/alerts using Amazon SNS on job failure/success
*  Provide ability to run backfills on your data

####<a id="pipelinebasiccmds">General Data Pipeline Commands</a>

*  _Define_ by defining the name of the data pipeline
*  _Import_ import in the data
*  _Activate_ to activate your pipeline

####<a id="pipelineusecase">Pipeline Use Case for Clickstreams</a>

1. S3 as Data Lake / Landing Zone
2. Amazon EMR as ETL grid (hive, pig)
3. Data Warehouse with Amazon Redshift

On a Data Pipeline, the activities looks like:

1. weblogs-bucket
2. logsProcess-ExtractTransform
3. goes to staging
4. goes to redshift
5. reports connect to redshift

For example, a PigActivity can do:

*  Can setup a schedule (e.g. every day, every 15 min)
*  Retry
*  On Fail/On Success
*  Define Input
*  Define Output
*  Runs on
