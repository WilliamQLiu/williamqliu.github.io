---
layout: post
title: ELK Stack
---


# {{ page.title }}

ELK stands for:

__Elasticsearch__ - Stores all of the logs
__Logstash__ - processes incoming logs
__Kibana__ - web interface for searching and visualizing logs, which are proxied through Nginx

__Filebeat__ - installed on client servers that will send their logs to Logstash. Filebeat is good for smaller log collection.
Other options for log shippers include __Fluentd__

## Installation

I installed via apt-get so I'm running these as systemctl, e.g.

	sudo systemctl start kibana
	sudo systemctl start elasticsearch
	sudo systemctl start nginx

If running locally, make sure to set `vm_max_map_count` to at least:

	sudo sysctl -w vm.max_map_count=262144

## Configs

Elasticsearch is configured through `/etc/elasticsearch/elasticsearch.yml`
Kibana is configured through `/etc/kibana/kibana.yml` or `/opt/kibana/config/kibana.yml`

### Elasticsearch Config

Setup your `/etc/elasticsearch/elasticsearch.yml` file so that it's

    network.host: localhost

Elasticsearch's configuration is in `elasticsearch/config/elasticsearch.yml`


### Kibana Config

Setup your `/opt/kibana/config/kibana.yml` file with
	
	server.host: "localhost"

### Nginx Config

For nginx, also install apache2-utils

	sudo apt-get install nginx apache2-utils

	# Create an admin user e.g. ('kibanaadmin', but use another name)
	sudo htpasswd -c /etc/nginx/htpasswd.users kibanaadmin

## Logstash Config

The Logstash configuration is stored in `logstash/config/logstash.yml`

# Let's see it work

So remember that we're trying to see the following:

1. Docker containers send logs using a log shipper like Filebeat or Fluentd. For our example, we'll send some logs through the command line
2. Logstash receives the logs and processes them
3. Elasticsearch searches logs
4. Kibana visualizes the logs

Try it with the docker-compose setup below

## Docker

So far the best docker-elk setup I've seen is here:

https://github.com/deviantony/docker-elk

## Ports

You'll see this on your localhost.

5601 shows Kibana
9300 Elasticsearch TCP transport
9200 Elasticsearch HTTP
5000 Logstash TCP input

### Test out the ports

Elasticsearch

	curl localhost:9200
	
	+will@xps ~ $ curl localhost:9200
	{
	  "name" : "vCF3oXg",
	  "cluster_name" : "docker-cluster",
	  "cluster_uuid" : "q49MoAYWRzSTgpnV2VU2rw",
	  "version" : {
	    "number" : "6.2.2",
	    "build_hash" : "10b1edd",
	    "build_date" : "2018-02-16T19:01:30.685723Z",
	    "build_snapshot" : false,
	    "lucene_version" : "7.2.1",
	    "minimum_wire_compatibility_version" : "5.6.0",
	    "minimum_index_compatibility_version" : "5.0.0"
	  },
	  "tagline" : "You Know, for Search"
	}

## Injecting Log Entries

First we need to inject some log entries.

    $ nc localhost 5000 < /path/to/logfile.log

## Setting up Kibana Index Pattern

After getting some logs, we'll create an index pattern via the Kibana API:

	$ curl -XPOST -D- 'http://localhost:5601/api/saved_objects/index-pattern' \
	    -H 'Content-Type: application/json' \
	    -H 'kbn-version: 6.2.2' \
	    -d '{"attributes":{"title":"logstash-*","timeFieldName":"@timestamp"}}'

