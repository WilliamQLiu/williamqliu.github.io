---
layout: post
title: Kafka
---


# {{ page.title }}

Notes from: https://kafka.apache.org/documentation/

## What is Kafka?

Apache Kafka is a distributed streaming platform.

A __streaming platform__ can:

* Publish and Subscribe to streams of records (similar to a message queue)
* Store streams of records in a fault-tolerant durable way (i.e. can continue to operate when portions break)
* Process streams of records as they occur

## Why use Kafka?

Kafka is usually built for two classes of applications:

* Building real-time streaming data pipelines that reliably get data between systems or applications
* Building real-time streaming applications that transform or react to the streams of data

## Kafka Concepts

* Kafka runs as a cluster on one or more servers that can span multiple datacenters
* The Kafka cluster stores streams of __records__ in categories called __topics__
* Each record has a key, a value, and a timestamp

## Kafka Tools

Kafka MirrorMaker - Copy files
Kafkacat - command line to test and debug; can produce, consume, list topics and partition information
Kafka Connect - Allow import/export of data (e.g. write to S3 kafka messages in JSON)

## Kafka Core APIs

Kafka has four core APIs.

* __Producer API__ - allows an application to publish a stream of records to one or more Kafka topics
* __Consumer API__ - allows an application to subscribe to one or more topics and process the stream of
                     records produced to them
* __Streams API__ - allows an application to act as a __stream processor__, consuming an input stream from
                    one or more topics and producing an output stream to one or more output topics
                    (i.e. transforms input streams to output streams)
* __Connector API__ - allows building and running reusable producers or consumers that connect Kafka
                      topics to existing applications or data systems (e.g. connector to a relational database
                      might capture every change to a table)

## Monitoring

For Kafka:

* Latency and Net I/O is important

## Configs

You can alter configs with the `./bin/kafka-configs.sh`.

To alter the details of a topic (e.g. when to log rotate segments), you can run:
`./bin/kafka-configs.sh --zookeeper zookeeper1:2181/kafka --alter --entity-type topics --entity-name events-1 --ad-config segments.ms=60000`

To delete a config, pass in the `--delete-config`

## Topics

The core abstraction that Kafka provides for a stream of records is the __topic__.

* A topic is a category or feed name where records are published.
* Topics are always __multi-subscriber__, meaning a topic can have zero, one, or many consumers that subscribe to
* the data written to it.
* For each topic, the Kafka cluster maintains a partitioned log that might look like:

Example Topic:

    Partition 0   0 1 2 3 4 5 6 7 8 9 10 11 12
    Partition 1   0 1 2 3 4 5 6 7 8 9
    Partition 2   0 1 2 3 4 5 6 7 8 9 19 11 12

Each __partition__ is an ordered, immutable sequence of records that is continually appended to.
The records in the partitions are each assigned a sequential id number called the __offset__ that uniquely
identifies each record within the partition. Offsets start at 0. Offsets keep increasing and are __immutable__.
The Kafka cluster durably persists all published records, whether or not they have been consumed by using
a configurable retention period (default: 7 days)
Unless you specify a key, partitions are not evenly distributed; they're randomly distributed.

### Advanced Topics

Use `kafka-topics.sh` and `kafka-configs.sh` for Kafka tools

kafka-topics.sh

* `__consumer_offsets` is an automatically generated topic holding all of our consumer offsets
* Can create a topic if it does not exist already `if-not-exists`
* You can alter the number of partitions (can only go up, not decrease the number of partitions). Need to delete and recreate for lowering
* Identify topics that have any overrides (configs added to the defaults) `--topics-with-overrides`
* Identify topics that are not in-sync with all replicas `--under-replicated-partitions`
* Identify topics without a leader replica `--unavailable-partitions`

kakfa-configs.sh

* Change the topics message retention period `--add-config retention.ms=3600000`
* Describe the configuration for a broker `--entity-type brokers --entity-name 0 --describe`
* Add a custom config to a broker `--add-config`
* Remove all custom configs on a broker `--alter --delete-config`

## Producers

__Producers__ are clients, usually applications that create a stream of messages. Data is written to disk
and replicated. Producers can wait for __acks__, which means it will wait for a number of acknowledgements before
confirming the successful delivery of a message.

Producers can specify __keys__ to indicated that a message will go to the same partition every time.
Producers send messages to Kafka __brokers__, which is able to intelligently replicate the data and elect a leader.

### Producer Tools

Create a dummy file `base64 /dev/random | head -c 10000 | egrep -ao "\w" | tr -d '\n' > myrandomfile.txt`
For performance testing, you can run: `./bin/kafka-producer-perf-test.sh --topic mytopic --num-records 1000 --throughput 10 --payload-file myrandomfile.txt --producer-props ack=1 bootstrap.server=kafka1:9092,kafka2:9092:kafka3:9092 --payload delimiter A`

## Consumers

__Consumers__ read messages and keep track of the offset.
When multiple messages are being read, a __consumer group__ can be formed so that each consumer is reading a different
message (i.e. no conflicts between reading many messages).

### Integrity of Consumers

The consumer goes out to the broker using a polling event. The consumer gets messages, then keeps track of the offset
to determine which message it read last. If you have too many threads going on at once, you'll get a concurrent
modification exception. To solve this, make sure there is only one consumer per thread (so we don't read too many
messages at once).

### Committed Offset vs Committed Message

A __committed offset__ is sent to Kafka by the consumer to acknowledge that it received AND processed all messages
in the partition up to that offset.

A __committed message__ is an acknowledgement sent by the leader replica, indicating to the broker that the
message has been committed (based on the acknowledgement setting).

### Offset Read vs Offset Processed

A consumer can start reading an offset, but never finish. This is a lost message.
To completely process the message, the consumer must commit the message.
Consumers may need to retry the records multiple times.

### Important Consumer Configs

Some important consumer configs are:

* __group.id__ - ensures that each consumer (if in a consumer group) has a unique group id
* __auto.offset.reset__ - tells the consumer what to do if the offset expires or doesn't exist anymore on the broker
  Can set to earliest (earliest offset in that partition), latest (latest offset in that partition), None (throws Exception
  if no other offset found)
* __enable.auto.commit__ - if true, periodically commits the consumer offset (otherwise your application will have to set it)
* __auto.commit.interval__ - if `enable.auto.commit` is set to true, then how often should we commit (in ms)

## Clusters and Brokers

A Kafka __cluster__ is made up of many Kafka brokers. A Kafka __broker__ is a server that receives messages from the
producer, assigns offsets and stores the messages on disk. Brokers replicate data across brokers in order to
create __fault tolerance__.

One broker is automatically selected as the __controller__, so this broker assigns the partitions for each broker
as well as monitors other brokers for failures (so it can fail over).

You can set __replication__ across brokers, meaning data is replicated across many brokers. With replication,
each Topic and Partition has a __Leader__ and `N` __Replica__ brokers (`N` being your __replication factor__ setting).
For example:

    Broker 1
    Topic A - Partition 0 (Leader)
    Topic A - Partition 2 (Replica)

    Broker 2
    Topic A - Partition 2 (Leader)
    Topic A - Partition 1 (Replica)

    Broker 3
    Topic A - Partition 1 (Leader)
    Topic A - Partition 0 (Replica)

### Broker Details

Each broker has an ID and this is very important. Zookeeper uses the broker id to help recover information.

* The broker config is found under `kafka/config/server.properties`
* server logs are in `kafka/logs/server.log` - Errors with the server itself (e.g. kafka does not come up, broker not respond)
* message logs are in `kafka/data/kafka`
* controller logs are in `kafka/logs/controller.log` tells us which broker is the controller (e.g. Broker 2 is the Controller)
* `state-change.log` shows us the changes in a controller (e.g. if a partition leader has been reassigned)
  In production environments, you want a broker that is responding before it gets elected a leader. In your configs,
  make sure to have `unclean.leader.election.enable=false`

## Zookeeper

__Zookeeper__ helps keep consensus within a cluster, meaning all brokers know which broker is the Controller,
the brokers are aware of each other, and what partition is is the Leader. Kafka requires Zookeeper in order to run.
Since Zookeeper helps with __Leader Election__, we must have an odd number of Zookeeper servers to keep __quorum__.

In a production setting, you want multiple zookeepers to create an __ensemble__.

### Zookeeper Details

We can use `kafka/bin/zookeeper-shell.sh` to run zookeeper specific commands.
To view ephemeral nodes in Zookeeper with `kafka/bin/zookeeper-shell.sh zookeeper1:2181/kafka ls get /controller`

`zookeeper.out` logs has your zookeeper errors (e.g. say zookeeper is not running)

## Install Kafka and Zookeeper

You can either install kafka and zookeeper using binaries or with containers.
Installing the binary takes more time, but allows you to run our programs as a service
(e.g. `sudo service zookeeper start`, `sudo service kafka start`) by adding to `/etc/init.d/kafka` and `/etc/init.d/zookeeper`
The below is with containers:

Add Docker to Your Package Repository

    curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -

    sudo add-apt-repository    "deb [arch=amd64] https://download.docker.com/linux/ubuntu \
       $(lsb_release -cs) \
       stable"

Update Packages and Install Docker

    sudo apt update

    sudo apt install -y docker-ce=18.06.1~ce~3-0~ubuntu

Add Your User to the Docker Group

    sudo usermod -a -G docker cloud_user

Install Docker Compose

    sudo -i
    curl -L https://github.com/docker/compose/releases/download/1.24.0/docker-compose-`uname -s`-`uname -m` -o /usr/local/bin/docker-compose
    chmod +x /usr/local/bin/docker-compose

Install Java

    sudo apt install -y default-jdk

Get the Kafka Binaries

    wget http://mirror.cogentco.com/pub/apache/kafka/2.2.0/kafka_2.12-2.2.0.tgz
    tar -xvf kafka_2.12-2.2.0.tgz

Create Your First Topic

    ./bin/kafka-topics.sh --zookeeper localhost:2181 --create --topic test --partitions 3 --replication-factor 1

Describe the Topic

    ./bin/kafka-topics.sh --zookeeper localhost:2181 --topic test --describe

### Important Locations

`./bin` is where the executables are (e.g. `./bin/kafka-topics.sh`)
`./config` is where the configurations are

## Kafka Bin Commands

There are a few Kafka commands and you can see the available options by not typing in any additional params or arguments.
For example `./bin/kafka-topics.sh` gives you:

    ./kafka-topics.sh
    Create, delete, describe, or change a topic.
    Option                                   Description
    ------                                   -----------
    --alter                                  Alter the number of partitions,
                                               replica assignment, and/or
                                               configuration for the topic.
    --config <String: name=value>            A topic configuration override for the
                                               topic being created or altered.The
                                               following is a list of valid
                                               configurations:
                                                 cleanup.policy
                                                 compression.type
                                                 delete.retention.ms
                                                 file.delete.delay.ms
                                                 flush.messages
                                                 flush.ms
                                                 follower.replication.throttled.
                                               replicas
                                                 index.interval.bytes
                                                 leader.replication.throttled.replicas
                                                 max.message.bytes
                                                 message.format.version
                                                 message.timestamp.difference.max.ms
                                                 message.timestamp.type
                                                 min.cleanable.dirty.ratio
                                                 min.compaction.lag.ms
                                                 min.insync.replicas
                                                 preallocate
                                                 retention.bytes
                                                 retention.ms
                                                 segment.bytes
                                                 segment.index.bytes
                                                 segment.jitter.ms
                                                 segment.ms
                                                 unclean.leader.election.enable
                                             See the Kafka documentation for full
                                               details on the topic configs.
    --create                                 Create a new topic.
    --delete                                 Delete a topic
    --delete-config <String: name>           A topic configuration override to be
                                               removed for an existing topic (see
                                               the list of configurations under the
                                               --config option).
    --describe                               List details for the given topics.
    --disable-rack-aware                     Disable rack aware replica assignment
    --force                                  Suppress console prompts
    --help                                   Print usage information.
    --if-exists                              if set when altering or deleting
                                               topics, the action will only execute
                                               if the topic exists
    --if-not-exists                          if set when creating topics, the
                                               action will only execute if the
                                               topic does not already exist
    --list                                   List all available topics.
    --partitions <Integer: # of partitions>  The number of partitions for the topic
                                               being created or altered (WARNING:
                                               If partitions are increased for a
                                               topic that has a key, the partition
                                               logic or ordering of the messages
                                               will be affected
    --replica-assignment <String:            A list of manual partition-to-broker
      broker_id_for_part1_replica1 :           assignments for the topic being
      broker_id_for_part1_replica2 ,           created or altered.
      broker_id_for_part2_replica1 :
      broker_id_for_part2_replica2 , ...>
    --replication-factor <Integer:           The replication factor for each
      replication factor>                      partition in the topic being created.
    --topic <String: topic>                  The topic to be create, alter or
                                               describe. Can also accept a regular
                                               expression except for --create option
    --topics-with-overrides                  if set when describing topics, only
                                               show topics that have overridden
                                               configs
    --unavailable-partitions                 if set when describing topics, only
                                               show partitions whose leader is not
                                               available
    --under-replicated-partitions            if set when describing topics, only
                                               show under replicated partitions
    --zookeeper <String: hosts>              REQUIRED: The connection string for
                                               the zookeeper connection in the form
                                               host:port. Multiple hosts can be
                                               given to allow fail-over.

Things to note:

* __ISR__ stands for __In Sync Replica__, which means that if a Partition Leader fails, then one of these
  In Sync Replicas can take over as Leader
* __Configs__ will show if this topic has a different set of configurations
* When specifying `--zookeeper`, you can specify one or all zookeepers (if all, use comma separated list)
* When running a producer, you can add in additional producer properties like `--producer-property acks=all`
* Remember that when running a consumer, unless you specify `from-beginning`, a consumer will only read new messages

## Consumer Groups

__Consumer Groups__ allow you multiple consumers to analyze messages in a topic in parallel without overlap.
For example, say you're producing messages to `mytopic`. You can then run two consumers (`consumer1` and `consumer2)
to read from `mytopic` and specify the same consumer group `myconsumergroup`. When a message is produced, the messages
are split across the consumers (i.e. some messages go to `consumer1` and some messages go to `consumer2`) without
any overlap. We can add and remove consumers as needed.

You can also create a new consumer group to read messages from a topic (e.g.
`bin/kafka-console-consumer.sh --bootstrap-server kafka3:9092 --topic test --group application1 --from-beginning`)

When you describe a consumer group, you'll see columns like:

* Topic Name
* Partition
* Current-Offset
* Log-End-Offset (Last Offset)
* Lag - number of messages behind that consumer hasn't read yet. This is based off all consumers
* Consumer-ID the specific consumer that is consuming messages

### kafka-consumer-groups

Use the `kafka-consumer-groups` command to change the configuration of our brokers and topics while the cluster
is up and running.

Documentation is here: https://kafka.apache.org/documentation/#basic_ops_consumer_group

Commands include:

* List all the consumer groups `bin/kafka-consumer-groups.sh --bootstrap-server kafka1:9092 --list`
* Describe a specific consumer group `describe --group mygroup`
* Describe active members of the group `bin/kafka-consumer-groups.sh --bootstrap-server kafka1:9092 --describe --group mygroup --members --verbose`
* Get the existing state of the group `--state`
* Delete a group `--delete --group mygroup`
* Reset offsets for a consumer group `--reset-offsets --group mygroup --topic mytopic --to-latest`. You
  can reset this for specific topics using `--topic` or all topics using `--all-topics`

### Offset reset options

`--reset-offsets` has following scenarios to choose from (atleast one scenario must be selected):

    --to-datetime <String: datetime> : Reset offsets to offsets from datetime. Format: 'YYYY-MM-DDTHH:mm:SS.sss'
    --to-earliest : Reset offsets to earliest offset.
    --to-latest : Reset offsets to latest offset.
    --shift-by <Long: number-of-offsets> : Reset offsets shifting current offset by 'n', where 'n' can be positive or negative.
    --from-file : Reset offsets to values defined in CSV file.
    --to-current : Resets offsets to current offset.
    --by-duration <String: duration> : Reset offsets to offset by duration from current timestamp. Format: 'PnDTnHnMnS'
    --to-offset : Reset offsets to a specific offset.

## Replicas

__Replicas__ are important in Kafka so that we can achieve __fault tolerance__. If one broker goes down, we want to
still ensure that there is zero data loss. Replicas are based off of Topic AND Partition.
There are two different types of replicas:

* __Leader Replica__ - is responsible for keeping all of the other replicas in sync
* __Follower Replica__ - is responsible for receiving messages that come in from the leader

The replicas also have offsets.

So what do these replicas do?

* Replicas are spread evenly amongst Brokers (e.g. replica 1 on broker2, replica 2 on broker1)
* Each partition is on a different Broker
* Put replicas on different racks (which is another setting on `server.properties` under `broker.rack=rack1`, etc)

### Replicas - In Sync vs Out of Sync

If the replica is not able to keep up (and the check is every 10 seconds), then the replica is labeled as 'out of sync'
instead of 'in sync'. In sync replicas can take over leader if the current leader goes down. Another way that
a replica may become 'out of sync' is if it cannot communicate with Zookeeper every 6 seconds.

## Requests

There are a few different types of __requests__, which can come from either a producer or a consumer. These types are:

* produce request - producer generates the request intended for the broker
* fetch request - both replicas and consumers use the fetch request to get data from leader (figures leader out from metadata request)
* metadata request - a cached list of topics that each broker has (what partitions, replicas and where the leader is for that message)

### Request Process

All of the above requests go through this type of process:

* Acceptor Thread - Creates a connection from the client (e.g. a produer or consumer) to the broker
* Processor Thread (aka the __Network Thread__) - Takes the requests from clients and places into a request queue
* IO Thread (aka the __Request Handler Thread__)  - Processes the requests from the request requeue
* Processor Thread (aka the __Network Thread__) - Picks up the request to send a response back to the client (the original producer or consumer) in a response queue

### Example Request

So how does this all work?

Say we have a produce request that gets written to the leader broker, but that data hasn't been replicated to our
replicas yet. If we make a fetch request and our replicas are not in-sync yet, even if we send the request to the
leader broker, the data is not yet available. The reasoning is that if the leader were to fail, our data has not been
replicated across yet so we do not have any data loss.

### Requests and Acks

If `acks` are set to all, then data is written to a buffer called __purgatory__ till the data is written to all
brokers, then it's available to all consumer clients.

Kafka is different than other databases in that it uses a __zero copy__ method to send messages to the clients (making
it extra efficient). It skips the network channel directly to the client instead of any intermediate buffers (like
saving to a local cache first). When making a request from a client, you can specify the amount of data (in case you
request too much, it'll try to store to memory so it might crash).

## Partitions

__Partitions__ are part of a topic. Partitions are split out amongst brokers to create resiliency and makes it easier
to consume/subscribe to multiple partitions. A partition can only be appended to (can only add, not delete).

Partitions are stored based on the `server.properties` under the `log.dirs` setting (e.g. `/data/kafka`). In each,
you will see something like:

* `.log` - data payload being stored (actual message itself) - see Segments Section
* `leader-epoch-checkpoint` - used to handle leader failures between replicas
* `.index` - represents the segment's __base offset__, has same name as the corresponding `.log` file. See Segments Section
* `.timeindex` -
* `.snapshot` - internal kafka snapshots

To view logs, you can run `./bin/kafka-run-class.sh kafka.tools.DumpLogSegments --print-data-log --files /data/kafka/events-1/00000000003064504069.log`
You will see the actual payload (i.e. what the message said) and its baseOffset

### Reassign Partitions

We can reassign partitions or change the replication factor for a partition and topics.

With `./bin/kafka-reassign-partitions.sh --zookeeper1:2181/kafka --generate --topics-to-move-json-file topics.json --broker-list 3,2,1`

With a JSON file:

    topics.json file
    {"topics":
     [{"topic": "test"}],
     "version":1
    }

## Segments

When Kafka writes to a partition, it writes to a __segment__, specifically the __active segment__. Once the
segment's size limit has been reached, a new segment is opened and that becomes the new active segment.
Segments are named by their base offset. The base offset of a segment is an offset greater than offsets in
previous segments and less than or equal to offsets in that segment.

So what is a segment?
On disk, a __partition is a directory__ and __each segment__ is made up of two files: an __index file__ and a
__log file__. A partition `events-1` with segments `00000000003064504069` and `00000000003065011416` looks like:

    tree kafka | head -n 6
    kafka
    ├── events-1
    │ ├── 00000000003064504069.index
    │ ├── 00000000003064504069.log
    │ ├── 00000000003065011416.index
    │ ├── 00000000003065011416.log

Segment logs are where messages are stored.

Has two fields:
    1. 4 Bytes: Relative Offset to the Base Offset
    2. 4 Bytes: Physical Position of the related Log Message (base offset + relative offset so we can get O(1) lookup time)
  The configuration `.index.interval.bytes` (4096 bytes by default) sets and index interval that describes how frequently
  (after how many bytes) an index entry will be added

## Data Delivery

Kafka is really concerned about __reliability__. Depending on the application, you can accidentally lose messages
or duplicate messages, causing issues.

* Kafka lets you choose between speed and reliability.
* Kafka guarantees __ACID__ as well as replication.
* Kafka guarantees that Message 2 will always be written after Message 1.
* When a message is committed (written to disk), that message will never be lost.

As a Kafka administrator, you can use the following ack tradeoffs on speed vs reliability:

* `acks=0` means no acknowledgement - guarantees are not made, retries are not attempted. Messages may be __duplicated__.
* `acks=1` means __leader acknowledgement__; guarantees the leader only; message lost if leader compromised
* `acks=all` means __all replicas acknowledge__; the leader will wait until all replicas acknowledge they have received the message

This can be set in a producer like:

`./bin/kafka-console-producer.sh --broker kafka1:9092 --topic test --producer-property acks=0`

## Security

We want secure connections between the producers, consumers, brokers, and zookeepers.

* We want to ensure that access is secure (__SSL__ or __SASL__). For __SASL__, we can choose between
  (__GSSAPI__ - kerberos windows AD, __PLAIN__ - username/password, __SCRAM-SHA-256__ more secure user/pass, __OAUTHBEARER__ json web tokens)
* We can also encrypt our data between our components in transit.

This will give you access so that certain users can change certain topics or brokers. All this is disabled by default.
There is minor performance degradation if SSL is activated.

## Schemas

You can use any data type in your Kafka clusters, but you may never know what consumers are trying to subscribe
to the messages so it's common to use a tool like __Avro__ to make sure that the consumers don't miss any data
within the messages themselves.

We can setup Avro to manage schemas over the life of the Kafka cluster. The producers serialize the data into Avro
and the consumers deserialize the data.

## Kafka Connect

__Kafka Connect__ is an API that comes with Kafka. It's a tool with already built connectors for many different
data sources, letting you get data in and out of a cluster quickly. Since Kafka Connect exposes a REST API,
this works well with other data sources. Kafka Connect works in standalone mode and in distributed mode.

An example would be if you have a MySQL table, you can stream that data into Kafka using Kafka Connect. You install
Kafka Connect plugin on the broker, then use the REST API to connect to the cluster.

### Kafka Connect Configs

You have the following important files:

* `config/connect-file-sink.properties` - Used for getting data OUT of the kafka cluster (e.g. a Hadoop sink)
* `config/connect-file-source.properties` - Used for getting data INTO the kafka cluster
* `config/connect-standalone.properties` - Kafka Connect in Standalone mode
* `config/connect-distributed.properties` - Kafka Connect in Distributed mode

Important configs in `connect-standalone` or `connect-distributed` modes:

* `bootstrap.servers=`

Important configs in `file-sink` or `file-source` properties:

* `connector.class` (e.g. FileStreamSink)

Run with: `./bin/connect-standalone.sh config/connect-standalone.properties config/connect-file-source.properties config/connect-file-sink.properties`


