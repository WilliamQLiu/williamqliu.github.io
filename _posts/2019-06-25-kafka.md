---
layout: post
title: Kafka
---


# {{ page.title }}

Notes from: https://kafka.apache.org/documentation/

## What is Kafka?

Apache Kafka is a distributed streaming platform.

A __streaming platform__ can:

* Publish and Subscribe to streams of records (similar to a message queue)
* Store streams of records in a fault-tolerant durable way (i.e. can continue to operate when portions break)
* Process streams of records as they occur

## Why use Kafka?

Kafka is usually built for two classes of applications:

* Building real-time streaming data pipelines that reliably get data between systems or applications
* Building real-time streaming applications that transform or react to the streams of data

## Kafka Concepts

* Kafka runs as a cluster on one or more servers that can span multiple datacenters
* The Kafka cluster stores streams of __records__ in categories called __topics__
* Each record has a key, a value, and a timestamp

## Kafka Tools

Kafka MirrorMaker - Copy files
Kafkacat - command line to test and debug; can produce, consume, list topics and partition information
Kafka Connect - Allow import/export of data (e.g. write to S3 kafka messages in JSON)

## Kafka Core APIs

Kafka has four core APIs.

* __Producer API__ - allows an application to publish a stream of records to one or more Kafka topics
* __Consumer API__ - allows an application to subscribe to one or more topics and process the stream of
                     records produced to them
* __Streams API__ - allows an application to act as a __stream processor__, consuming an input stream from
                    one or more topics and producing an output stream to one or more output topics
                    (i.e. transforms input streams to output streams)
* __Connector API__ - allows building and running reusable producers or consumers that connect Kafka
                      topics to existing applications or data systems (e.g. connector to a relational database
                      might capture every change to a table)

## Monitoring

For Kafka:

* Latency and Net I/O is important


## Topics

The core abstraction that Kafka provides for a stream of records is the __topic__.

* A topic is a category or feed name where records are published.
* Topics are always __multi-subscriber__, meaning a topic can have zero, one, or many consumers that subscribe to
* the data written to it.
* For each topic, the Kafka cluster maintains a partitioned log that might look like:

Example Topic:

    Partition 0   0 1 2 3 4 5 6 7 8 9 10 11 12
    Partition 1   0 1 2 3 4 5 6 7 8 9
    Partition 2   0 1 2 3 4 5 6 7 8 9 19 11 12

Each __partition__ is an ordered, immutable sequence of records that is continually appended to.
The records in the partitions are each assigned a sequential id number called the __offset__ that uniquely
identifies each record within the partition. Offsets start at 0. Offsets keep increasing and are __immutable__.
The Kafka cluster durably persists all published records, whether or not they have been consumed by using
a configurable retention period (default: 7 days)
Unless you specify a key, partitions are not evenly distributed; they're randomly distributed.

## Producers

__Producers__ are clients, usually applications that create a stream of messages. Data is written to disk
and replicated. Producers can wait for __acks__, which means it will wait for a number of acknowledgements before
confirming the successful delivery of a message.

Producers can specify __keys__ to indicated that a message will go to the same partition every time.
Producers send messages to Kafka __brokers__, which is able to intelligently replicate the data and elect a leader.

## Consumers

__Consumers__ read messages and keep track of the offset.
When multiple messages are being read, a __consumer group__ can be formed so that each consumer is reading a different
message (i.e. no conflicts between reading many messages).

## Clusters and Brokers

A Kafka __cluster__ is made up of many Kafka brokers. A Kafka __broker__ is a server that receives messages from the
producer, assigns offsets and stores the messages on disk. Brokers replicate data across brokers in order to
create __fault tolerance__.

One broker is automatically selected as the __controller__, so this broker assigns the partitions for each broker
as well as monitors other brokers for failures (so it can fail over).

You can set __replication__ across brokers, meaning data is replicated across many brokers. With replication,
each Topic and Partition has a __Leader__ and `N` __Replica__ brokers (`N` being your __replication factor__ setting).
For example:

    Broker 1
    Topic A - Partition 0 (Leader)
    Topic A - Partition 2 (Replica)

    Broker 2
    Topic A - Partition 2 (Leader)
    Topic A - Partition 1 (Replica)

    Broker 3
    Topic A - Partition 1 (Leader)
    Topic A - Partition 0 (Replica)

## Zookeeper

__Zookeeper__ helps keep consensus within a cluster, meaning all brokers know which broker is the Controller,
the brokers are aware of each other, and what partition is is the Leader. Kafka requires Zookeeper in order to run.
Since Zookeeper helps with __Leader Election__, we must have an odd number of Zookeeper servers to keep __quorum__.

In a production setting, you want multiple zookeepers to create an __ensemble__.

## Install Kafka

Add Docker to Your Package Repository

    curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -

    sudo add-apt-repository    "deb [arch=amd64] https://download.docker.com/linux/ubuntu \
       $(lsb_release -cs) \
       stable"

Update Packages and Install Docker

    sudo apt update

    sudo apt install -y docker-ce=18.06.1~ce~3-0~ubuntu

Add Your User to the Docker Group

    sudo usermod -a -G docker cloud_user

Install Docker Compose

    sudo -i
    curl -L https://github.com/docker/compose/releases/download/1.24.0/docker-compose-`uname -s`-`uname -m` -o /usr/local/bin/docker-compose
    chmod +x /usr/local/bin/docker-compose

Install Java

    sudo apt install -y default-jdk

Get the Kafka Binaries

    wget http://mirror.cogentco.com/pub/apache/kafka/2.2.0/kafka_2.12-2.2.0.tgz
    tar -xvf kafka_2.12-2.2.0.tgz

Create Your First Topic

    ./bin/kafka-topics.sh --zookeeper localhost:2181 --create --topic test --partitions 3 --replication-factor 1

Describe the Topic

    ./bin/kafka-topics.sh --zookeeper localhost:2181 --topic test --describe


