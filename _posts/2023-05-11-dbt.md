---
layout: post
title: DBT
---


# {{ page.title }}

## Cheatsheet Commands

* `dbt run` executes the compiled sql model files against the current `target` database
  - `dbt run --full-refresh` will treat incremental models as table models
  - `dbt select --select my_selected_model` to only run that table
* `dbt build` will run models, test tests, snapshot snapshots, seed seeds
* `dbt test` will test your models
* `dbt docs generate` to create documents and `dbt docs serve` to start a webserver on port `8080`

## Summary

__dbt__ (data build tool) is a transformation workflow that helps modularize and centralize your analytics code.
Collaborate on data models, version them, test, and document. dbt compiles and runs yours analytics
code against your data platform, enabling a single source of truth for metrics, insights, and business
definitions.

### Advantages:

* Avoid writing boilerplate __DML__ (Data Manipulation Language, a class of SQL statements used to query,
edit, and delete row-level data, e.g. SELECT, INSERT, DELETE, UPDATE)
* Avoid writing boilerplate __DDL__ (Data Definition Language, a group of SQL statements that you can execute
to manage database objects, including tables, views, and more)
* Write business logic with a SQL `select` statement, a Python DataFrame, and dbt handles the materialization
* Leverage metadata to find long-running models
* Use incremental models
* Write DRYer code using __macros__, __hooks__, and __package management__

## Quicklinks

* [dbt Docs](https://docs.getdbt.com/)
* [dbt Core](https://github.com/dbt-labs/dbt-core), an open-source cli tool

## DBT Fundamentals

### Traditional Data Teams

* Data Engineers - make sure data is where it should be (i.e. orchestrate moving data) - builds dashboards, sql, excel
  - Knows how to get data around
* Data Analysts - works with business decision makers, queries tables that data engineers built - etl, python, java, orchestration
  - Knows what to build

### ETL vs ELT

ETL (Extract Transform Load) - more traditional process

* Extract data
* Manipulate that data
* Load that data so data analysts can query that data


ELT (Extract, Load, Transform)

Data can be transformed directly in the database - no need to extract and load repeatedly

* Once we get the data into the warehouse
* Transform the data after

Introduces the idea of an __analytics engineer__

### Analytics Engineer

Owns the Transformation of raw data up to the BI Layer

Modern Data Team:

* Data Engineer - build custom data ingestion integrations, manage overall pipeline orchestration, develop and
  deploy machine learning endpoints, build and maintain the data platform, data warehouse performance optimization
* Analytics Engineer - provide clean, transformed data ready for analysis, apply software engineering practices to
  analytics code (e.g. version control, testing, continuous integration), maintain data docs and definitions,
  train business users on how to use a data platform data visualization tools
* Data Analytics - deep insights work (e.g. why did churn spike last month? what are the best acquisition channels?),
  work with business users to understand data requirements, build critical dashboards, forecasting

## Configs

### `dbt_project.yml`

Every project needs a `dbt_project.yml` file (how dbt knows a directory is a dbt project).
We use the current working directory or you can set the `--project-dir` flag. An example file looks like:

```
[name](project-configs/name): string

[config-version](project-configs/config-version): 2
[version](project-configs/version): version

[profile](project-configs/profile): profilename

[model-paths](project-configs/model-paths): [directorypath]
[seed-paths](project-configs/seed-paths): [directorypath]
[test-paths](project-configs/test-paths): [directorypath]
[analysis-paths](project-configs/analysis-paths): [directorypath]
[macro-paths](project-configs/macro-paths): [directorypath]
[snapshot-paths](project-configs/snapshot-paths): [directorypath]
[docs-paths](project-configs/docs-paths): [directorypath]
[asset-paths](project-configs/asset-paths): [directorypath]

[target-path](project-configs/target-path): directorypath
[log-path](project-configs/log-path): directorypath
[packages-install-path](project-configs/packages-install-path): directorypath

[clean-targets](project-configs/clean-targets): [directorypath]

[query-comment](project-configs/query-comment): string

[require-dbt-version](project-configs/require-dbt-version): version-range | [version-range]

[quoting](project-configs/quoting):
  database: true | false
  schema: true | false
  identifier: true | false

models:
  [<model-configs>](model-configs)

seeds:
  [<seed-configs>](seed-configs)

snapshots:
  [<snapshot-configs>](snapshot-configs)

sources:
  [<source-configs>](source-configs)

tests:
  [<test-configs>](test-configs)

vars:
  [<variables>](/docs/build/project-variables)

[on-run-start](project-configs/on-run-start-on-run-end): sql-statement | [sql-statement]
[on-run-end](project-configs/on-run-start-on-run-end): sql-statement | [sql-statement]

[dispatch](project-configs/dispatch-config):
  - macro_namespace: packagename
    search_order: [packagename]
```

### `.dbtignore`

Specify files that should be entirely ignored by dbt (similar to a `.gitignore`)

```
# .dbtignore

# ignore individual .py files
not-a-dbt-model.py
another-non-dbt-model.py

# ignore all .py files
**.py

# ignore all .py files with "codegen" in the filename
*codegen*.py
```

## Modeling

Normally we create:

* source tables
* intermediate tables/views
* final tables

In dbt, modeling are just SQL Select Statements.

* Models live in the `models` directory.
* each model has a one-to-one relationship with a table or view in the data warehouse
* you configure how dbt builds your models
* dbt will handle the DDL/DML

### Model Example

__dim_customers.sql__

See how we have a few CTEs (Common Table Expression)

```
with customers as (
  select
    id as customer_id,
    first_name,
    last_name
  from raw.jaffle_shop.customers
),
orders as (
  select
    id as order_id,
    user_id as customer_id,
    order_date,
    status
  from raw.jaffle_shop.orders
),
customer_orders as (
  select
    customer_id,
    min(order_date) as first_order_date,
    max(order_date) as most_recent_order_date,
    count(order_id) as number_of_orders
  from orders
  group by 1
),
final as (
  select
    customers.customer_id,
    customers.first_name,
    customers.last_name,
    customer_orders.first_order_date,
    customer_orders.most_recent_order_date,
    coalesce(customer_orders.number_of_orders, 0) as number_of_orders
  from customers
  left join customer_orders using (customer_id)
)

select * from final
```

with a config block

```
{{ config (
    materialized="table"
)}}
```


